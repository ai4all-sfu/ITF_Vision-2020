{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "Real-Time-Application with Image Capture.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUPvKnSTXAi4",
    "colab_type": "text"
   },
   "source": [
    "## Multiple Face Detection and Emotion Recognition on Live Stream \n",
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8h7B8hW2XAi5",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "611728f3-f7b3-4ee6-eda8-fad2997dc931"
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFPuklmSXAjB",
    "colab_type": "text"
   },
   "source": [
    "### Load pre-trained model for face detection and emotion recognition "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PX6TOKOrXAjC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## TODO Load Neural Network Facial Emotion Model saved while training\n",
    "#load model\n",
    "model =\n",
    "#load weights\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "## Face Detection model loaded\n",
    "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEvHtehCiN1b",
    "colab_type": "text"
   },
   "source": [
    "# Privacy Policy\n",
    "Please be advised that the image you capture from yourself in the next part will be send to the Google colab server. Although it will clear shortly after the session ends. Also, if you decide not to upload your own photo you can upload a demo photo under the name of `photo.jpg` and jump to the *Apply model on captured image * step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A50sSWq5Htt5",
    "colab_type": "text"
   },
   "source": [
    "### Capture image of yourself for emotion recognition "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "SucxddsPhOmj",
    "colab": {}
   },
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcE7H1WAH_1L"
   },
   "source": [
    "### Save the image"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "buJCl90WhNfq",
    "colab": {}
   },
   "source": [
    "from IPython.display import Image\n",
    "try:\n",
    "  filename = take_photo()\n",
    "  print('Saved to {}'.format(filename))\n",
    "  \n",
    "  # Show the image which was just taken.\n",
    "  display(Image(filename))\n",
    "except Exception as err:\n",
    "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "  # grant the page permission to access it.\n",
    "  print(str(err))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzFaKWh6XAjJ",
    "colab_type": "text"
   },
   "source": [
    "### Apply model on captured image"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-JqEBmrmXAjO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "from google.colab.patches import cv2_imshow\n",
    "test_img = cv2.imread('photo.jpg')\n",
    "gray_img = cv2.imread('photo.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "\n",
    "\n",
    "for (x,y,w,h) in faces_detected:\n",
    "    cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n",
    "    roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image\n",
    "    roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "    img_pixels = image.img_to_array(roi_gray)\n",
    "    img_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "    img_pixels /= 255\n",
    "\n",
    "    ## TODO Predict model on the captures image\n",
    "    predictions =\n",
    "    ############################################\n",
    "\n",
    "    # TODO find max indexed array to find predicted emotion\n",
    "    max_index =\n",
    "    #######################################################\n",
    "\n",
    "    emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    predicted_emotion = emotions[max_index]\n",
    "    print(predicted_emotion)\n",
    "\n",
    "    cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "resized_img = cv2.resize(test_img, (1000, 700))\n",
    "cv2_imshow(resized_img)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9v7bfS9ZXAjU",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}